@inproceedings{TopicalChanges,
author="Rieger, Jonas and Kai-Robin Lange and Jonathan Flossdorf and Carsten Jentsch",
title="Dynamic change detection in topics based on rolling {LDAs}",
booktitle="Proceedings of the Text2Story'22 Workshop",
year = "2022",
}



@inproceedings{RollingLDA,
    title = "{RollingLDA}: {A}n Update Algorithm of {L}atent {D}irichlet {A}llocation to Construct Consistent Time Series from Textual Data",
    author = {Rieger, Jonas  and
      Jentsch, Carsten  and
      Rahnenf{\"u}hrer, J{\"o}rg},
    booktitle = "Findings Proceedings of the 2021 EMNLP-Conference",
    year = "2021",
}

@article {PRR2,
    AUTHOR = {Jentsch, Carsten and Lee, Eun Ryung and Mammen, Enno},
     TITLE = {Poisson reduced-rank models with an application to political
              text data},
   JOURNAL = {Biometrika},
    VOLUME = {108},
      YEAR = {2021},
    NUMBER = {2},
}

@article {PRR1,
    AUTHOR = {Jentsch, Carsten and Lee, Eun Ryung and Mammen, Enno},
     TITLE = {Time-dependent {P}oisson reduced rank models for political
              text data analysis},
              JOURNAL = {Computational Statistics \& Data Analysis},
    VOLUME = {142},
      YEAR = {2020},
}

@INPROCEEDINGS{zeitenwenden,
  title = {Zeitenwenden: {{Detecting}} Changes in the {{German}} Political Discourse},
  author = {Lange, Kai-Robin and Rieger, Jonas and Benner, Niklas and Jentsch, Carsten},
  year = {2022},
  journal = {Proceedings of the 2nd Workshop on Computational Linguistics for Political Text Analysis},
}

@inproceedings{LDAPrototype,
  title = {Improving {{Latent Dirichlet Allocation}}: {{On Reliability}} of the {{Novel Method LDAPrototype}}},
  shorttitle = {Improving {{Latent Dirichlet Allocation}}},
  booktitle = {Natural {{Language Processing}} and {{Information Systems}}},
  author = {Rieger, Jonas and Rahnenf{\"u}hrer, J{\"o}rg and Jentsch, Carsten},
  year = {2020},
  langid = {english},
  keywords = {Machine learning,Similarity,Stability,Stochastic,Topic model},
  file = {C\:\\Users\\kalange\\Zotero\\storage\\VW9JGB8P\\Rieger et al. - 2020 - Improving Latent Dirichlet Allocation On Reliabil.pdf}
}

@inproceedings{Hamilton,
    title = "Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change",
    author = "Hamilton, William L.  and
      Leskovec, Jure  and
      Jurafsky, Dan",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    year = "2016",
}

@inproceedings{huDiachronicSenseModeling2019,
  title = {Diachronic {{Sense Modeling}} with {{Deep Contextualized Word Embeddings}}: {{An Ecological View}}},
  shorttitle = {Diachronic {{Sense Modeling}} with {{Deep Contextualized Word Embeddings}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Hu, Renfen and Li, Shen and Liang, Shichen},
  editor = {Korhonen, Anna and Traum, David and M{\`a}rquez, Llu{\'i}s},
  year = {2019},
  month = jul,
  pages = {3899--3908},
  publisher = {Association for Computational Linguistics},
  address = {Florence, Italy},
  doi = {10.18653/v1/P19-1379},
  urldate = {2025-01-11},
  abstract = {Diachronic word embeddings have been widely used in detecting temporal changes. However, existing methods face the meaning conflation deficiency by representing a word as a single vector at each time period. To address this issue, this paper proposes a sense representation and tracking framework based on deep contextualized embeddings, aiming at answering not only what and when, but also how the word meaning changes. The experiments show that our framework is effective in representing fine-grained word senses, and it brings a significant improvement in word change detection task. Furthermore, we model the word change from an ecological viewpoint, and sketch two interesting sense behaviors in the process of language evolution, i.e. sense competition and sense cooperation.},
  file = {C:\Users\kalange\Zotero\storage\M4CFZB4W\Hu et al. - 2019 - Diachronic Sense Modeling with Deep Contextualized.pdf}
}
@misc{mikolovEfficientEstimationWord2013,
  title = {Efficient {{Estimation}} of {{Word Representations}} in {{Vector Space}}},
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  year = {2013},
  month = sep,
  number = {arXiv:1301.3781},
  eprint = {1301.3781},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2022-11-28},
  abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\kalange\\Zotero\\storage\\R3AYXSXB\\Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Ve.pdf;C\:\\Users\\kalange\\Zotero\\storage\\YIR54JZJ\\1301.html}
}

@article{bleiLatentDirichletAllocation,
  title = {Latent {{Dirichlet Allocation}}},
  author = {Blei, David M},
  year = {2003},
  abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
  langid = {english},
  file = {C:\Users\kalange\Zotero\storage\Q746ZPTB\Blei - Latent Dirichlet Allocation.pdf}
}

@inproceedings{riegerImprovingLatentDirichlet2020,
  title = {Improving {{Latent Dirichlet Allocation}}: {{On Reliability}} of the {{Novel Method LDAPrototype}}},
  shorttitle = {Improving {{Latent Dirichlet Allocation}}},
  booktitle = {Natural {{Language Processing}} and {{Information Systems}}},
  author = {Rieger, Jonas and Rahnenf{\"u}hrer, J{\"o}rg and Jentsch, Carsten},
  editor = {M{\'e}tais, Elisabeth and Meziane, Farid and Horacek, Helmut and Cimiano, Philipp},
  year = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {118--125},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-51310-8_11},
  abstract = {A large number of applications in text data analysis use the Latent Dirichlet Allocation (LDA) as one of the most popular methods in~topic modeling. Although the instability of the LDA is mentioned sometimes, it is usually not considered systematically. Instead, an LDA is often selected from a small set of LDAs using heuristic means or human codings. Then, conclusions are often drawn based on the to some extent arbitrarily selected model. We present the novel method LDAPrototype, which takes the instability of the LDA into account, and show that by systematically selecting an LDA it improves the reliability of the conclusions drawn from the result and thus provides better reproducibility. The improvement coming from this selection criterion is unveiled by applying the proposed methods to an example corpus consisting of texts published in a German quality newspaper over one month.},
  isbn = {978-3-030-51310-8},
  langid = {english},
  keywords = {Machine learning,Similarity,Stability,Stochastic,Topic model},
  file = {C:\Users\kalange\Zotero\storage\VW9JGB8P\Rieger et al. - 2020 - Improving Latent Dirichlet Allocation On Reliabil.pdf}
}
